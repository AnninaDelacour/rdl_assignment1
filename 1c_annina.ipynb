{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fee5be5f-746e-4bae-a285-fb55b95ca277",
   "metadata": {},
   "source": [
    "# Exercise 1c - Testing Transformer\n",
    "\n",
    "The objective of this task is to simulate how AI can assist in expanding research papers, generating missing explanations, or rephrasing content in a meaningful way.\n",
    "<br>\n",
    "\n",
    "- <b>Group 3:</b> Cesar Laura, Ecker Annina, Dilly Julian\n",
    "- <b>Section of Paper:</b> \"Multi-Head Attention + Scaled Dot Production\"\n",
    "<br>\n",
    "<br>\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Note: Each of us worked on all tasks independently. We later discussed our findings and merged the best/most representative parts with eachother in one Notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34ede63a-e565-4365-aa5e-a38739d15c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt\n",
    "#!pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef9d656c-e7a0-4289-8285-2ef75685152d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8049951-d32b-4abc-8f69-d9438be34b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/annina/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import set_seed\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "import transformers\n",
    "\n",
    "import torch\n",
    "\n",
    "import textwrap\n",
    "\n",
    "import bert_score\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cbed8e3-7a28-4fc0-938b-a078797511bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157ed6c5-6a42-4bf6-b82d-160ff0f9c772",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:#CCD7E9;background-color:#CCD7E9\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d41b495-9a6e-45f4-95e7-da7dd23809a9",
   "metadata": {},
   "source": [
    "# Model: GPT2-XL\n",
    "\n",
    "Source: __[Huggingface - OpenAI-Community/GPT2](https://huggingface.co/openai-community/gpt2)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26e0f432-ad94-40a1-8d8d-c9ceb36b01f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model='gpt2-xl', device=\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1bc07f59-359f-4108-a459-e26acaba4afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Please provide a detailed explanation of the concept of Multi-Head Attention based on the following research excerpt:\n",
    "\n",
    "'Instead of performing a single attention function with dmodel-dimensional keys, values and queries, \n",
    "we found it beneficial to linearly project the queries, keys and values h times with different, learned \n",
    "linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of \n",
    "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional \n",
    "output values. These are concatenated and once again projected, resulting in the final values.\n",
    "Multi-head attention allows the model to jointly attend to information from different representation \n",
    "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
    "\n",
    "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use \n",
    "d_k = d_v = d_model/h = 64. Due to the reduced dimension of each head, the total computational cost \n",
    "is similar to that of single-head attention with full dimensionality.'\n",
    "\n",
    "Expand on this explanation by describing how Multi-Head Attention enhances deep learning models and why it is beneficial.\n",
    "\n",
    "____________________________________________________________________\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "53ddd063-963a-484e-85cb-c3df3223a5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please provide a detailed explanation of the concept of Multi-Head Attention based on the following research excerpt:\n",
      "\n",
      "'Instead of performing a single attention function with dmodel-dimensional keys, values and queries, \n",
      "we found it beneficial to linearly project the queries, keys and values h times with different, learned \n",
      "linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of \n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional \n",
      "output values. These are concatenated and once again projected, resulting in the final values.\n",
      "Multi-head attention allows the model to jointly attend to information from different representation \n",
      "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "\n",
      "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use \n",
      "d_k = d_v = d_model/h = 64. Due to the reduced dimension of each head, the total computational cost \n",
      "is similar to that of single-head attention with full dimensionality.'\n",
      "\n",
      "Expand on this explanation by describing how Multi-Head Attention enhances deep learning models and why it is beneficial.\n",
      "\n",
      "____________________________________________________________________\n",
      "\n",
      "The above paper describes an approach for multi-headed attention using linear projection. The authors propose a method for multi-headed attention which uses linear projection to combine multiple representations into one representation. This technique has been used successfully in previous studies (e.g., [1]–[3]). However, there have not been any attempts to apply this technique to deep neural networks. In this study, we present a novel implementation of multi-headed attention using linear projection. We show that our algorithm can be applied to both unsupervised and supervised learning tasks. Our results demonstrate that multi-headed attention improves performance on several important tasks including object detection, image classification, and language modeling. Furthermore, we find that multi-headed attention can improve performance on some common problems such as image segmentation, speech recognition, and text generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = generator(prompt, \n",
    "                    max_new_tokens=200, \n",
    "                    num_return_sequences=1, # num_return_sequences = 1: default generation; 3-5 means more creative output with alt answers\n",
    "                    temperature=0.2, # higher temperatures enable the model to experiment more with its answers (0.2-0.5 for scientific answers)\n",
    "                    top_k=5, # number of tokens to get a deterministic result -> more tokens (e.g., 50-100) the more creative the text\n",
    "                    top_p=0.9, # model only considers words with highest probability until, e.g., 90% threshold of prob_sum is achieved\n",
    "                    repetition_penalty=1.2, # 1-1.2 good default values; >1.5 forces more variation but also unnatural sounding answers; <1.0 can result in repeated sentences\n",
    "                    do_sample=False, # do_sample makes text creative; if precise answers only, then do_sample=False -> deterministic\n",
    "                    truncation=True,\n",
    "                    pad_token_id=50256\n",
    "                )\n",
    "\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d937dea-5db1-4322-9648-cc36ceda7a37",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Preliminary Results\n",
    "\n",
    "This generated output presents a fabricated summary of a non-existent research study, falsely suggesting that the excerpt refers to an original paper authored by the model’s creators. While it correctly mentions \"multi-headed attention\" and \"linear projection,\" the rest of the content is hallucinated — it references unspecified prior studies ([1]–[3]) and falsely claims novel implementations and results in tasks like object detection or speech recognition without any factual grounding. These claims are not part of the original excerpt and are unsupported by evidence.\n",
    "<br><br>\n",
    "This output fails the task. Instead of expanding on the provided excerpt to explain the mechanism of Multi-Head Attention, it fabricates research claims and introduces unrelated tasks, confusing the reader and misrepresenting the original content. It is not reliable for scientific explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784ba597-88b6-4937-896f-8a72f9638e07",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4b3e4ee-e5c4-47b3-8dda-b0d7874504a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = \"\"\"\n",
    "In transformer models, Multi-Head Attention is used to process queries, keys, and values by projecting them into lower-dimensional spaces and applying parallel attention mechanisms. This allows the model to capture information from different representation subspaces.\n",
    "\n",
    "Explain in scientific detail how Multi-Head Attention enhances deep learning models, focusing on its computational benefits, ability to process contextual information, and impact on model performance. Avoid repeating the summary above and expand upon it in technical language.\n",
    "\n",
    "____________________________________________________________________\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d63e872e-6381-4c34-a826-b506091f7299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In transformer models, Multi-Head Attention is used to process queries, keys, and values by projecting them into lower-dimensional spaces and applying parallel attention mechanisms. This allows the model to capture information from different representation subspaces.\n",
      "\n",
      "Explain in scientific detail how Multi-Head Attention enhances deep learning models, focusing on its computational benefits, ability to process contextual information, and impact on model performance. Avoid repeating the summary above and expand upon it in technical language.\n",
      "\n",
      "____________________________________________________________________\n",
      "\n",
      "3.2.1. Neural Networks with Multiple Inputs\n",
      "\n",
      "Multi-Head Attention can be applied to neural networks that have multiple input layers (e.g., convolutional or recurrent). In this case, the model will use a single attention layer for each of these inputs. The model then uses multi-head attention to extract features from the data.\n",
      "\n",
      "The following figure shows an example of a network with two input layers: one for the first image and another for the second image. Each input layer has a single attention layer.\n",
      "\n",
      "Figure 3.1: Example of a Convolutional Neural Network with Two Input Layers\n",
      "\n",
      "Note that the output layer does not have any attention layer. It simply outputs the value of the first image.\n",
      "\n",
      "To understand why this works, consider the following scenario:\n",
      "\n",
      "A user asks you to predict whether a particular person is male or female. You are given a set of images of people and asked to classify which ones belong to which gender. For instance, if you were given a set of pictures of men and women, you would say \"male\" or \"female\". If you were given a set of pictures of both men and women, you would say \"both\".\n",
      "\n",
      "If you had a neural network with two input layers, you could train it to recognize the gender of the faces in the set of images. Then, when you ask it to predict whether a face belongs to a certain gender, it would look at all the images of faces and decide whether they belong to either male or female based on their gender.\n",
      "\n",
      "This is similar to what happens in the previous section where we trained a neural network to recognize the gender of the faces in the set of images. However, instead of using a single attention layer, we now use a multi-head attention layer.\n",
      "\n",
      "When training a neural network with multiple input layers, the model will use a single attention layer for each of these inputs. The\n"
     ]
    }
   ],
   "source": [
    "outputs2 = generator(\n",
    "    prompt2,\n",
    "    max_new_tokens=400,           \n",
    "    temperature=0.2,             \n",
    "    top_k=30,                  \n",
    "    top_p=0.9,                  \n",
    "    repetition_penalty=1.2,      \n",
    "    do_sample=False,           \n",
    "    pad_token_id=50256,\n",
    "    eos_token_id=50256\n",
    ")\n",
    "print(outputs2[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e9019e-93e1-4880-95e1-8e7988fe51c4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Preliminary Results\n",
    "\n",
    "The generated text is highly repetitive and hallucinatory. Rather than delivering a technical explanation of Multi-Head Attention, it spirals into an unrealistic and irrelevant list of hypothetical course topics, repeatedly mentioning \"modeling the behavior of nonlinear systems\" in various redundant contexts. It fails to address the original prompt and does not provide any meaningful information about Multi-Head Attention, its mechanics, or computational benefits. The content lacks coherence, factual accuracy, and practical relevance, and is an example of a failed generation.<br><br>\n",
    "\n",
    "This output incorrectly frames Multi-Head Attention as a mechanism applied to multiple input layers like images in convolutional neural networks, which is a misinterpretation of its purpose. The explanation diverges from the correct technical context of attention mechanisms in transformers and instead describes a flawed analogy involving gender classification. It confuses the role of attention layers and inaccurately claims that attention is applied to entire input layers, rather than within a sequence of tokens. The response introduces hallucinated examples and misleads in both structure and function, ultimately failing to answer the original prompt accurately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424a0f86-cabd-439d-8a4c-ead790e14b69",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "37e8f632-7dd3-4a5e-bc40-edc1ea79cbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt3 = \"\"\"\n",
    "Explain in scientific detail how Multi-Head Attention works in transformer models, \n",
    "and how it enhances performance through parallel attention heads. Discuss its computational \n",
    "benefits and its ability to process information from multiple subspaces simultaneously.\n",
    "____________________________________________________________________\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "67db1e4e-efee-4312-ae9f-bed4ee36229d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Explain in scientific detail how Multi-Head Attention works in transformer models, \n",
      "and how it enhances performance through parallel attention heads. Discuss its computational \n",
      "benefits and its ability to process information from multiple subspaces simultaneously.\n",
      "____________________________________________________________________\n",
      "The following is a summary of the lecture notes for this course:\n",
      "Course Description: This course will cover the theory and application of multi-head attention in transformer models. The focus will be on the use of multi-head attention in modeling the behavior of transformers. Topics include: (1) the theoretical foundations of multi-head attention; (2) the implementation of multi-head attention in transformer models; (3) the role of multi-head attention in model selection; (4) the use of multi-head attention in modeling the behavior of transformers; (5) the use of multi-head attention in modeling the behavior of nonlinear systems; (6) the use of multi-head attention in modeling the behavior of nonlinear systems with finite state space; (7) the use of multi-head attention in modeling the behavior of nonlinear systems with infinite state space; (8) the use of multi-head attention in modeling the behavior of nonlinear systems with continuous state space; (9) the use of multi-head attention in modeling the behavior of nonlinear systems with discrete state space; (10) the use of multi-head attention in modeling the behavior of nonlinear systems with continuous time domain; (11) the use of multi-head attention in modeling the behavior of nonlinear systems with discrete time domain; (12) the use of multi-head attention in modeling the behavior of nonlinear systems with continuous time domain with finite state space; (\n"
     ]
    }
   ],
   "source": [
    "outputs3 = generator(\n",
    "    prompt3,\n",
    "    max_new_tokens=300,           \n",
    "    temperature=0.2,              \n",
    "    top_k=30,        \n",
    "    top_p=0.9,               \n",
    "    repetition_penalty=1.2,     \n",
    "    do_sample=False, \n",
    "    pad_token_id=50256,\n",
    "    eos_token_id=50256\n",
    ")\n",
    "print(outputs3[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3693d993-beaa-455e-b96e-eec32b3e9f03",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b752db-3076-4e51-a919-061ca0c4eed8",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Preliminary Results\n",
    "\n",
    "The generated output does not fulfill the prompt’s request for a scientific explanation of Multi-Head Attention. Instead, it spirals into a repetitive and off-topic summary resembling fictitious lecture notes, with excessive enumeration and no technical content about transformer models or attention mechanisms. There is no mention of key concepts like query/key/value projections, parallel attention heads, or computational efficiency. Additionally, the output exhibits clear signs of hallucination and lacks coherence, making it unusable for any scientific or educational purpose.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f978cd-d31a-4018-986d-1f804086337b",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:#CCD7E9;background-color:#CCD7E9\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ccde8a-fc51-4c74-b674-69bd064aba69",
   "metadata": {},
   "source": [
    "# Model: Falcon-7b-Instruct\n",
    "\n",
    "Source: __[Huggingface - Tiiuae/Falcon-7B-Instruct](https://huggingface.co/tiiuae/falcon-7b-instruct)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7d4b009-0202-433d-9b45-b961428d3b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████| 2/2 [00:11<00:00,  5.88s/it]\n"
     ]
    }
   ],
   "source": [
    "generator2 = pipeline(\"text-generation\", model=\"microsoft/phi-2\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "92e1fd70-4916-4202-aed9-3622fb879d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt4 = \"\"\"\n",
    "Please provide a detailed explanation of the concept of Multi-Head Attention based on the following research excerpt:\n",
    "\n",
    "'Instead of performing a single attention function with dmodel-dimensional keys, values and queries, \n",
    "we found it beneficial to linearly project the queries, keys and values h times with different, learned \n",
    "linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of \n",
    "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional \n",
    "output values. These are concatenated and once again projected, resulting in the final values.\n",
    "Multi-head attention allows the model to jointly attend to information from different representation \n",
    "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
    "\n",
    "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use \n",
    "d_k = d_v = d_model/h = 64. Due to the reduced dimension of each head, the total computational cost \n",
    "is similar to that of single-head attention with full dimensionality.'\n",
    "\n",
    "Expand on this explanation by describing how Multi-Head Attention enhances deep learning models and why it is beneficial.\n",
    "\n",
    "____________________________________________________________________\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "127ff9f1-0af9-4463-a2ca-59cf4527be41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please provide a detailed explanation of the concept of Multi-Head Attention based on the following research excerpt:\n",
      "\n",
      "'Instead of performing a single attention function with dmodel-dimensional keys, values and queries, \n",
      "we found it beneficial to linearly project the queries, keys and values h times with different, learned \n",
      "linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of \n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional \n",
      "output values. These are concatenated and once again projected, resulting in the final values.\n",
      "Multi-head attention allows the model to jointly attend to information from different representation \n",
      "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "\n",
      "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use \n",
      "d_k = d_v = d_model/h = 64. Due to the reduced dimension of each head, the total computational cost \n",
      "is similar to that of single-head attention with full dimensionality.'\n",
      "\n",
      "Expand on this explanation by describing how Multi-Head Attention enhances deep learning models and why it is beneficial.\n",
      "\n",
      "____________________________________________________________________\n",
      "Solution 1:\n",
      "----------------------\n",
      "The concept of Multi-Head Attention introduces multiple attention mechanisms within a single layer of a neural network architecture. Instead of using a single attention mechanism with all input data being processed through one set of weights (keys), values, and query vectors, Multi-Head Attention employs several independent attention mechanisms called \"heads.\" Each head operates independently but learns its own unique weightings for processing specific parts of the input sequence. This approach enables the model to focus on various aspects simultaneously, capturing diverse representations across the entire input space. By combining the outputs of individual heads, the overall model gains enhanced capabilities to capture complex patterns and relationships present in the data. The benefits of employing Multi-Head Attention include improved generalization performance, increased capacity to handle long sequences, and better utilization of available resources due to parallel computation among multiple attention heads.\n",
      "\n",
      "Follow-up Exercise 2:\n",
      "---------------------------\n",
      "Explain the process of applying Dropout Regularization during training and discuss its impact on preventing overfitting. Provide an example scenario where Dropout can be effectively utilized.\n",
      "\n",
      "Solution 2:\n",
      "----------------\n",
      "Dropout regularization involves randomly setting a fraction of neurons in a hidden layer to zero during training while keeping them active during inference. During training, when dropout is applied, some neurons are temporarily deactivated, which helps prevent co-adaptation between neurons and reduces their dependency on other inputs. As a result, the model becomes more robust against noise and less prone to memorizing the\n"
     ]
    }
   ],
   "source": [
    "outputs4 = generator2(\n",
    "    prompt4,\n",
    "    max_new_tokens=300,           \n",
    "    temperature=0.2,              \n",
    "    top_k=30,        \n",
    "    top_p=0.9,               \n",
    "    repetition_penalty=1.2,     \n",
    "    do_sample=False, \n",
    "    pad_token_id=50256,\n",
    "    eos_token_id=50256\n",
    ")\n",
    "print(outputs4[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911858a3-99ff-449c-8eb4-557ddcaca131",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Preliminary Results\n",
    "\n",
    "The model produced a coherent and technically accurate explanation of Multi-Head Attention. It correctly emphasized that instead of using a single attention mechanism, multiple independent attention heads process different aspects of the input in parallel. This allows the model to capture diverse patterns and relationships across the sequence, which enhances representation capacity.\n",
    "<br>\n",
    "Key benefits like improved generalization, handling of long sequences, and parallel computation were highlighted – aligning well with core theoretical insights from transformer architectures.\n",
    "<br>\n",
    "Unexpectedly, the output also included a follow-up task about Dropout Regularization. While this is unrelated, it shows the model's inclination to generate structured responses (e.g., \"Solution 1\", \"Solution 2\"). This is not harmful but may be trimmed or controlled for focused outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab290bbf-3b9e-4277-bf02-b6a5e447b8a5",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1e49e332-5cab-4570-b591-dab260221f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Explain in scientific detail how Multi-Head Attention works in transformer models, \n",
      "and how it enhances performance through parallel attention heads. Discuss its computational \n",
      "benefits and its ability to process information from multiple subspaces simultaneously.\n",
      "____________________________________________________________________\n",
      "Solution:\n",
      "Multi-Head Attention is a key component of Transformer models that allows the model to focus on different positions or \"heads\" within the input sequence at each step during training. This enables the model to capture various aspects of the data more effectively by attending to different parts concurrently. The concept behind multi-head attention can be understood as follows:\n",
      "1. Key Points: In traditional self-attention mechanisms, the same set of weights are applied to all elements in the input sequence for computing the dot product between them. However, with multi-head attention, we introduce multiple independent linear projections (Q, K, V) into separate channels, allowing us to attend to distinct subsets of features present in the input sequence. Each head learns to weigh these features differently based on their relevance to the current position being attended to.\n",
      "2. Parallel Processing: By dividing the input sequence into several smaller chunks called query/key pairs, each represented independently by one head, we enable parallel processing across multiple heads. During computation, each head attends to its own queries and keys while also considering the values associated with those queries and keys. This parallelism significantly reduces the overall time complexity required for computations compared to performing sequential operations on the entire input sequence.\n",
      "3. Weight Sharing: Although each head operates independently, they share common parameters such as weight matrices Wq, Wh, and Wo. These shared parameters allow the model to learn global representations efficiently without explicitly sharing every single parameter value among\n"
     ]
    }
   ],
   "source": [
    "outputs5 = generator2(\n",
    "    prompt3,\n",
    "    max_new_tokens=300,           \n",
    "    temperature=0.2,              \n",
    "    top_k=30,        \n",
    "    top_p=0.9,               \n",
    "    repetition_penalty=1.2,     \n",
    "    do_sample=False, \n",
    "    pad_token_id=50256,\n",
    "    eos_token_id=50256\n",
    ")\n",
    "print(outputs5[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad37ca93-eead-4152-a666-af5c9d173df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b796550-062a-4765-a1be-3ee3918d1eba",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Preliminary Results\n",
    "\n",
    "The generated output provides a solid technical overview of how Multi-Head Attention functions within transformer models. It highlights the use of independent attention heads, each attending to different parts of the input in parallel, which allows the model to capture diverse aspects of the data more effectively. The explanation correctly details key mechanisms such as query/key/value projections, parallel processing, and shared weights, emphasizing both computational efficiency and representational depth. While the explanation is accurate and avoids hallucinations, it ends abruptly, missing a conclusion or discussion of real-world benefits. Nonetheless, it successfully conveys the technical foundation and practical advantages of Multi-Head Attention.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89859bae-2505-472d-8627-dde4080b3523",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cf3bab88-2c48-4ba5-ba64-e3748cb2d441",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_simple = \"\"\"\n",
    "Please explain the concept of Multi-Head Attention in a simple and easy-to-understand way. \n",
    "Base your explanation on the following research excerpt, and **add any important missing context** that would help a beginner fully understand how Multi-Head Attention works and why it is useful.\n",
    "\n",
    "Excerpt:\n",
    "'Instead of performing a single attention function with dmodel-dimensional keys, values and queries, \n",
    "we found it beneficial to linearly project the queries, keys and values h times with different, learned \n",
    "linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of \n",
    "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional \n",
    "output values. These are concatenated and once again projected, resulting in the final values.\n",
    "Multi-head attention allows the model to jointly attend to information from different representation \n",
    "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
    "\n",
    "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use \n",
    "d_k = d_v = d_model/h = 64. Due to the reduced dimension of each head, the total computational cost \n",
    "is similar to that of single-head attention with full dimensionality.'\n",
    "\n",
    "**Explain in your own words, in a beginner-friendly way, and add any helpful examples.**\n",
    "\n",
    "____________________________________________________________________\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6084ac8d-7612-4546-a816-590f2e6edf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please explain the concept of Multi-Head Attention in a simple and easy-to-understand way. \n",
      "Base your explanation on the following research excerpt, and **add any important missing context** that would help a beginner fully understand how Multi-Head Attention works and why it is useful.\n",
      "\n",
      "Excerpt:\n",
      "'Instead of performing a single attention function with dmodel-dimensional keys, values and queries, \n",
      "we found it beneficial to linearly project the queries, keys and values h times with different, learned \n",
      "linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of \n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional \n",
      "output values. These are concatenated and once again projected, resulting in the final values.\n",
      "Multi-head attention allows the model to jointly attend to information from different representation \n",
      "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "\n",
      "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use \n",
      "d_k = d_v = d_model/h = 64. Due to the reduced dimension of each head, the total computational cost \n",
      "is similar to that of single-head attention with full dimensionality.'\n",
      "\n",
      "**Explain in your own words, in a beginner-friendly way, and add any helpful examples.**\n",
      "'''\n",
      "\n",
      "\n",
      "# Solution 1:\n",
      "\"\"\"\n",
      "The multi-headed attention mechanism helps us pay more attention to specific parts of our input data by allowing multiple \"heads\" (or separate focus points) to look at different aspects of the same thing simultaneously. This can be especially useful when dealing with complex inputs like images or natural language text where there may be many different features or patterns that need to be considered together.\n",
      "\n",
      "To illustrate this idea, imagine you have an image of a cat sitting on a windowsill. If you were trying to identify what's happening in the picture using only one set of visual filters (like color channels), you might miss some key details because they don't fit neatly into those pre-defined categories. But if you had several sets of filters looking for different things - say, motion detection, edge detection, and texture analysis - you'd be much better equipped to capture all the relevant information about the scene.\n",
      "\n",
      "Similarly, in multi-headed attention, instead of just having one big filter that looks at everything in the input sequence, we create multiple smaller filters (\"attention heads\") that specialize in focusing on different types of information. Each head has its own set of learnable parameters that allow it to weigh certain pieces of evidence more heavily than others based on their relevance to the task at hand. By combining the outputs of all the heads, we end up with a richer, more nuanced understanding of the input overall.\n",
      "\n",
      "Here's an example implementation of\n"
     ]
    }
   ],
   "source": [
    "outputs6 = generator2(\n",
    "    prompt_simple,\n",
    "    max_new_tokens=300,           \n",
    "    temperature=0.2,              \n",
    "    top_k=30,        \n",
    "    top_p=0.9,               \n",
    "    repetition_penalty=1.2,     \n",
    "    do_sample=False, \n",
    "    pad_token_id=50256,\n",
    "    eos_token_id=50256\n",
    ")\n",
    "print(outputs6[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f846556e-aed2-4b22-b4ed-095540ce621f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Preliminary Results\n",
    "\n",
    "The generated explanation of Multi-Head Attention presents the concept in a simple, accessible manner, well-suited for beginners. It uses a high-level, intuitive analogy involving a cat and various visual filters, effectively illustrating how multiple attention heads can focus on different aspects of the same input to gain a richer understanding. This analogy is a creative and engaging way to demystify a complex concept, particularly for those unfamiliar with neural network mechanisms.\n",
    "<br>\n",
    "The output goes beyond the provided excerpt by adding helpful context: it clearly explains the role of individual attention heads and their specialization in processing distinct types of information. This insight allows beginners to grasp the benefit of using multiple heads, namely, improved comprehension of complex data through parallel processing and diverse focus.\n",
    "<br><br>\n",
    "Importantly, the output avoids hallucinations and does not contain any repetitive or irrelevant content, maintaining both factual accuracy and readability throughout.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc028384-22fc-46b2-84e0-b502a14f6157",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db76432d-7c6d-495e-a0f1-7b70578b645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_advanced = \"\"\"\n",
    "Please provide a detailed and technically accurate explanation of Multi-Head Attention, \n",
    "based on the following research excerpt. Do not repeat the excerpt, but expand on the key ideas, \n",
    "add missing concepts, and explain why Multi-Head Attention improves model performance\n",
    "in transformer-based architectures.\n",
    "\n",
    "Excerpt:\n",
    "'Instead of performing a single attention function with dmodel-dimensional keys, values and queries, \n",
    "we found it beneficial to linearly project the queries, keys and values h times with different, learned \n",
    "linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of \n",
    "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional \n",
    "output values. These are concatenated and once again projected, resulting in the final values.\n",
    "Multi-head attention allows the model to jointly attend to information from different representation \n",
    "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
    "\n",
    "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use \n",
    "d_k = d_v = d_model/h = 64. Due to the reduced dimension of each head, the total computational cost \n",
    "is similar to that of single-head attention with full dimensionality.'\n",
    "\n",
    "Explain the mechanism, benefits, and provide relevant technical details to fill any knowledge gaps.\n",
    "Avoid code examples, focus only on explanation and analogies.\n",
    "\n",
    "____________________________________________________________________\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0701314-05b6-4948-8829-5ca6bca8428a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please provide a detailed and technically accurate explanation of Multi-Head Attention, \n",
      "based on the following research excerpt. Do not repeat the excerpt, but expand on the key ideas, \n",
      "add missing concepts, and explain why Multi-Head Attention improves model performance\n",
      "in transformer-based architectures.\n",
      "\n",
      "Excerpt:\n",
      "'Instead of performing a single attention function with dmodel-dimensional keys, values and queries, \n",
      "we found it beneficial to linearly project the queries, keys and values h times with different, learned \n",
      "linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of \n",
      "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional \n",
      "output values. These are concatenated and once again projected, resulting in the final values.\n",
      "Multi-head attention allows the model to jointly attend to information from different representation \n",
      "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
      "\n",
      "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use \n",
      "d_k = d_v = d_model/h = 64. Due to the reduced dimension of each head, the total computational cost \n",
      "is similar to that of single-head attention with full dimensionality.'\n",
      "\n",
      "Explain the mechanism, benefits, and provide relevant technical details to fill any knowledge gaps.\n",
      "Avoid code examples, focus only on explanation and analogies.\n",
      "\n",
      "____________________________________________________________________\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "# %% [markdown]\n",
      "# ## Solution\n",
      "#\n",
      "# The idea behind multi-headed attention is simple - instead of using one set of weights for all inputs (as in standard self-attention), you can have multiple sets of weights, which will allow your network to learn more complex representations by attending to various parts of the input simultaneously. This has been shown to improve performance significantly compared to traditional models like LSTMs and GRUs.\n",
      "#\n",
      "# In our implementation, we first split the original query, key, value vectors into `n` smaller ones, where `n` is equal to the number of attention heads. We do this by applying linear transformations to them, so that they become `dk`, `dk`, and `dv` dimensional vectors, respectively. Then, we apply the same attention calculation as before, but now there are `n` separate calculations happening concurrently. Finally, we combine the results of all these calculations together and return an output vector of size `d_model`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputs7 = generator2(\n",
    "    prompt_advanced,\n",
    "    max_new_tokens=500,           \n",
    "    temperature=0.2,              \n",
    "    top_k=40,        \n",
    "    top_p=0.9,               \n",
    "    repetition_penalty=1.2,     \n",
    "    do_sample=False, \n",
    "    pad_token_id=50256,\n",
    "    eos_token_id=50256\n",
    ")\n",
    "print(outputs7[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372c6342-036f-48b3-b142-57b7dd37756a",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Preliminary Results\n",
    "\n",
    "The model provides a technically correct and coherent explanation of Multi-Head Attention. The explanation correctly outlines that multiple attention calculations are performed in parallel across these \"heads,\" and that their outputs are concatenated and projected again to form the final output.<br>\n",
    "The response also highlights a key benefit: enabling the model to focus on different parts of the input simultaneously, thereby capturing more complex relationships. Additionally, the explanation mentions that this approach leads to improved performance over traditional architectures such as LSTMs and GRUs.<br>\n",
    "\n",
    "However, some technical depth is missing. Specifically, it does not explain why splitting into smaller dimensions keeps computational cost manageable, nor does it mention the role of Scaled Dot-Product Attention or representation subspaces — important concepts in understanding the efficiency and expressiveness of Multi-Head Attention. Also, while the comparison to LSTMs is fair, it lacks a direct link to how attention improves long-range dependency modeling, which is central to its success.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8492b9-42fe-486d-9bb0-7343d028830b",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:#CCD7E9;background-color:#CCD7E9\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e2af37-8632-4010-9d2f-28a53ebe25d8",
   "metadata": {},
   "source": [
    "# Model: Vectara - Hallucination Eval Model\n",
    "Source: __[Huggingface - Vectara/Hallucination Evaluation Model](https://huggingface.co/vectara/hallucination_evaluation_model)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35357579-240d-40ab-a81a-1f13729938a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type HHEMv2Config to instantiate a model of type HHEMv2. This is not supported for all configurations of models and can yield errors.\n",
      "You are using a model of type HHEMv2Config to instantiate a model of type HHEMv2. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "prompt_template = \"<pad> Determine if the hypothesis is true given the premise?\\n\\nPremise: {text1}\\n\\nHypothesis: {text2}\"\n",
    "\n",
    "input_prompts = [\n",
    "    prompt_template.format(text1=original_text.strip(), text2=answer.strip()) \n",
    "    for answer in generated_answers\n",
    "]\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model='vectara/hallucination_evaluation_model',\n",
    "    tokenizer=AutoTokenizer.from_pretrained('google/flan-t5-base'),\n",
    "    trust_remote_code=True,\n",
    "    device=\"mps\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04330cc8-e0d8-4b3c-81ce-4f4ba1b5f49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = \"\"\"\n",
    "Instead of performing a single attention function with dmodel-dimensional keys, values and queries, \n",
    "we found it beneficial to linearly project the queries, keys and values h times with different, learned \n",
    "linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of \n",
    "queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional \n",
    "output values. These are concatenated and once again projected, resulting in the final values.\n",
    "Multi-head attention allows the model to jointly attend to information from different representation \n",
    "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
    "\n",
    "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use \n",
    "d_k = d_v = d_model/h = 64. Due to the reduced dimension of each head, the total computational cost \n",
    "is similar to that of single-head attention with full dimensionality.\n",
    "\"\"\"\n",
    "\n",
    "generated_answers = [\n",
    "    \"\"\"The idea behind multi-headed attention is simple - instead of using one set of weights for all inputs \n",
    "    (as in standard self-attention), you can have multiple sets of weights, which will allow your network to l\n",
    "    earn more complex representations by attending to various parts of the input simultaneously. \n",
    "    This has been shown to improve performance significantly compared to traditional models like LSTMs and GRUs.\n",
    "    In our implementation, we first split the original query, key, value vectors into `n` smaller ones, where `n` is equal \n",
    "    to the number of attention heads. We do this by applying linear transformations to them, so that they become `dk`, `dk`, \n",
    "    and `dv` dimensional vectors, respectively. Then, we apply the same attention calculation as before, but now there are `n` \n",
    "    separate calculations happening concurrently. \n",
    "    Finally, we combine the results of all these calculations together and return an output vector of size `d_model`.\"\"\",\n",
    "    \n",
    "      \"\"\"The multi-headed attention mechanism helps us pay more attention to specific parts of our input data by \n",
    "    allowing multiple \"heads\" (or separate focus points) to look at different aspects of the same thing \n",
    "    simultaneously. This can be especially useful when dealing with complex inputs \n",
    "    like images or natural language text where there may be many different \n",
    "    features or patterns that need to be considered together.\n",
    "    To illustrate this idea, imagine you have an image of a cat sitting on a windowsill. \n",
    "    If you were trying to identify what's happening in the picture using only one set of visual filters \n",
    "    (like color channels), you might miss some key details because they don't fit neatly into those pre-defined categories. \n",
    "    But if you had several sets of filters looking for different things - say, motion detection, \n",
    "    edge detection, and texture analysis - you'd be much better equipped to capture all the relevant information about the scene.\n",
    "    Similarly, in multi-headed attention, instead of just having one big filter that looks at everything in the input sequence, \n",
    "    we create multiple smaller filters (\"attention heads\") that specialize in focusing on different types of information. \n",
    "    Each head has its own set of learnable parameters that allow it to weigh certain pieces of evidence more heavily \n",
    "    than others based on their relevance to the task at hand. By combining the outputs of all the heads, we end up with a richer, \n",
    "    more nuanced understanding of the input overall.\"\"\",\n",
    "    \n",
    "    \"\"\"Multi-Head Attention is a key component of Transformer models that allows the model to focus on different positions or\n",
    "    \"heads\" within the input sequence at each step during training. This enables the model to capture various aspects of the \n",
    "    data more effectively by attending to different parts concurrently. The concept behind multi-head attention can be understood \n",
    "    as follows:\n",
    "    1. Key Points: In traditional self-attention mechanisms, the same set of weights are applied to all elements in the input \n",
    "    sequence for computing the dot product between them. However, with multi-head attention, we introduce multiple independent \n",
    "    linear projections (Q, K, V) into separate channels, allowing us to attend to distinct subsets of features present in the \n",
    "    input sequence. Each head learns to weigh these features differently based on their relevance to the \n",
    "    current position being attended to.\n",
    "    2. Parallel Processing: By dividing the input sequence into several smaller chunks called query/key pairs, each represented \n",
    "    independently by one head, we enable parallel processing across multiple heads. During computation, each head attends to its own \n",
    "    queries and keys while also considering the values associated with those queries and keys. This parallelism significantly reduces \n",
    "    the overall time complexity required for computations compared to performing sequential operations on the entire input sequence.\n",
    "    3. Weight Sharing: Although each head operates independently, they share common parameters such as weight matrices Wq, Wh, and Wo. \n",
    "    These shared parameters allow the model to learn global representations efficiently without explicitly sharing every single parameter\n",
    "    value among\"\"\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c14bfab9-d8bc-4faf-be9c-6ec21ced6240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (531 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "results = classifier(input_prompts, top_k=None)\n",
    "\n",
    "\n",
    "final_results = []\n",
    "for idx, res_pair in enumerate(results):\n",
    "    for entry in res_pair:\n",
    "        if entry[\"label\"] == \"hallucinated\":\n",
    "            halluc_score = round(entry[\"score\"], 4)\n",
    "        if entry[\"label\"] == \"consistent\":\n",
    "            consistent_score = round(entry[\"score\"], 4)\n",
    "    \n",
    "    final_results.append({\n",
    "        \"Answer ID\": idx + 1,\n",
    "        \"Consistent Score\": consistent_score,\n",
    "        \"Hallucination Score\": halluc_score,\n",
    "        \"Prediction\": \"✅ Consistent\" if consistent_score > halluc_score else \"❌ Hallucinated\",\n",
    "        \"Generated Answer\": generated_answers[idx].strip()\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "271cbf4d-82ea-4c18-97cd-aa13dc8b0768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer ID</th>\n",
       "      <th>Consistent Score</th>\n",
       "      <th>Hallucination Score</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Generated Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0377</td>\n",
       "      <td>0.9623</td>\n",
       "      <td>❌ Hallucinated</td>\n",
       "      <td>The idea behind multi-headed attention is simp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>0.9844</td>\n",
       "      <td>❌ Hallucinated</td>\n",
       "      <td>The multi-headed attention mechanism helps us ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0237</td>\n",
       "      <td>0.9763</td>\n",
       "      <td>❌ Hallucinated</td>\n",
       "      <td>Multi-Head Attention is a key component of Tra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Answer ID  Consistent Score  Hallucination Score      Prediction  \\\n",
       "0          1            0.0377               0.9623  ❌ Hallucinated   \n",
       "1          2            0.0156               0.9844  ❌ Hallucinated   \n",
       "2          3            0.0237               0.9763  ❌ Hallucinated   \n",
       "\n",
       "                                    Generated Answer  \n",
       "0  The idea behind multi-headed attention is simp...  \n",
       "1  The multi-headed attention mechanism helps us ...  \n",
       "2  Multi-Head Attention is a key component of Tra...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_final = pd.DataFrame(final_results)\n",
    "display(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f6bda374-e86e-4fd5-a6ae-2e475da602c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_answers_single = [\n",
    "    \"\"\"The idea behind multi-headed attention is simple - instead of using one set of weights for all inputs \n",
    "    (as in standard self-attention), you can have multiple sets of weights, which will allow your network to l\n",
    "    earn more complex representations by attending to various parts of the input simultaneously.\"\"\",\n",
    "    \n",
    "    \"\"\"In our implementation, we first split the original query, key, value vectors into `n` smaller ones, where `n` is equal \n",
    "    to the number of attention heads.\"\"\" ,\n",
    "    \n",
    "    \"\"\"We do this by applying linear transformations to them, \n",
    "    so that they become `dk`, `dk`,  and `dv` dimensional vectors, respectively.\"\"\",\n",
    "    \n",
    "      \"\"\"The multi-headed attention mechanism helps us pay more attention to specific parts of our input data by \n",
    "    allowing multiple \"heads\" (or separate focus points) to look at different aspects of the same thing \n",
    "    simultaneously.\"\"\",\n",
    "    \n",
    "    \"\"\"This can be especially useful when dealing with complex inputs like images or natural language text \n",
    "    where there may be many different features or patterns that need to be considered together.\"\"\",\n",
    "    \n",
    "    \"\"\"To illustrate this idea, imagine you have an image of a cat sitting on a windowsill.\"\"\",\n",
    "    \n",
    "    \"\"\"If you were trying to identify what's happening in the picture using only one set of visual filters \n",
    "    (like color channels), you might miss some key details because they don't fit neatly into those pre-defined categories.\"\"\",\n",
    "    \n",
    "    \"\"\"Each head has its own set of learnable parameters that allow it to weigh certain pieces of evidence more heavily \n",
    "    than others based on their relevance to the task at hand.\"\"\",\n",
    "    \n",
    "    \"\"\"Multi-Head Attention is a key component of Transformer models that allows the model to focus on different positions or\n",
    "    \"heads\" within the input sequence at each step during training.\"\"\",\n",
    "    \n",
    "    \"\"\"This enables the model to capture various aspects of the data more effectively by attending to different parts concurrently.\"\"\",\n",
    "    \n",
    "    \"\"\"In traditional self-attention mechanisms, the same set of weights are applied to all elements in the input \n",
    "    sequence for computing the dot product between them.\"\"\",\n",
    "    \n",
    "    \"\"\"However, with multi-head attention, we introduce multiple independent linear projections (Q, K, V) into separate channels, \n",
    "    allowing us to attend to distinct subsets of features present in the input sequence.\"\"\",\n",
    "    \n",
    "    \"\"\"Each head learns to weigh these features differently based on their relevance to the current position being attended to.\"\"\",\n",
    "    \n",
    "    \"\"\"By dividing the input sequence into several smaller chunks called query/key pairs, each represented independently by one head, \n",
    "    we enable parallel processing across multiple heads.\"\"\"\n",
    "    \n",
    "    \"\"\"Although each head operates independently, they share common parameters such as weight matrices Wq, Wh, and Wo.\"\"\",\n",
    "    \n",
    "    \"\"\"These shared parameters allow the model to learn global representations efficiently without explicitly sharing every single parameter\n",
    "    value among\"\"\"\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "531bf551-bd18-433d-9d09-31534447dcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompts2 = [\n",
    "    prompt_template.format(text1=original_text.strip(), text2=answer.strip()) \n",
    "    for answer in generated_answers_single\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5915dd96-640c-49e9-b576-52d3dcf47fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = classifier(input_prompts2, top_k=None)\n",
    "\n",
    "\n",
    "final_results2 = []\n",
    "for idx, res_pair in enumerate(results2):\n",
    "    for entry in res_pair:\n",
    "        if entry[\"label\"] == \"hallucinated\":\n",
    "            halluc_score = round(entry[\"score\"], 4)\n",
    "        if entry[\"label\"] == \"consistent\":\n",
    "            consistent_score = round(entry[\"score\"], 4)\n",
    "    \n",
    "    final_results2.append({\n",
    "        \"Answer ID\": idx + 1,\n",
    "        \"Consistent Score\": consistent_score,\n",
    "        \"Hallucination Score\": halluc_score,\n",
    "        \"Prediction\": \"✅ Consistent\" if consistent_score > halluc_score else \"❌ Hallucinated\",\n",
    "        \"Generated Answer\": generated_answers_single[idx].strip()\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35bf68df-bbba-4102-8567-0c4f7cb36e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Answer ID</th>\n",
       "      <th>Consistent Score</th>\n",
       "      <th>Hallucination Score</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Generated Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0112</td>\n",
       "      <td>0.9888</td>\n",
       "      <td>❌ Hallucinated</td>\n",
       "      <td>The idea behind multi-headed attention is simp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0542</td>\n",
       "      <td>0.9458</td>\n",
       "      <td>❌ Hallucinated</td>\n",
       "      <td>In our implementation, we first split the orig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0849</td>\n",
       "      <td>0.9151</td>\n",
       "      <td>❌ Hallucinated</td>\n",
       "      <td>We do this by applying linear transformations ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.6472</td>\n",
       "      <td>0.3528</td>\n",
       "      <td>✅ Consistent</td>\n",
       "      <td>The multi-headed attention mechanism helps us ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0624</td>\n",
       "      <td>0.9376</td>\n",
       "      <td>❌ Hallucinated</td>\n",
       "      <td>This can be especially useful when dealing wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0099</td>\n",
       "      <td>0.9901</td>\n",
       "      <td>❌ Hallucinated</td>\n",
       "      <td>To illustrate this idea, imagine you have an i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.9956</td>\n",
       "      <td>❌ Hallucinated</td>\n",
       "      <td>If you were trying to identify what's happenin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>❌ Hallucinated</td>\n",
       "      <td>Each head has its own set of learnable paramet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.2404</td>\n",
       "      <td>0.7596</td>\n",
       "      <td>❌ Hallucinated</td>\n",
       "      <td>Multi-Head Attention is a key component of Tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.8093</td>\n",
       "      <td>0.1907</td>\n",
       "      <td>✅ Consistent</td>\n",
       "      <td>This enables the model to capture various aspe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>0.9861</td>\n",
       "      <td>❌ Hallucinated</td>\n",
       "      <td>In traditional self-attention mechanisms, the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.0779</td>\n",
       "      <td>0.9221</td>\n",
       "      <td>❌ Hallucinated</td>\n",
       "      <td>However, with multi-head attention, we introdu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.0641</td>\n",
       "      <td>0.9359</td>\n",
       "      <td>❌ Hallucinated</td>\n",
       "      <td>Each head learns to weigh these features diffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.9930</td>\n",
       "      <td>❌ Hallucinated</td>\n",
       "      <td>By dividing the input sequence into several sm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.1086</td>\n",
       "      <td>0.8914</td>\n",
       "      <td>❌ Hallucinated</td>\n",
       "      <td>These shared parameters allow the model to lea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Answer ID  Consistent Score  Hallucination Score      Prediction  \\\n",
       "0           1            0.0112               0.9888  ❌ Hallucinated   \n",
       "1           2            0.0542               0.9458  ❌ Hallucinated   \n",
       "2           3            0.0849               0.9151  ❌ Hallucinated   \n",
       "3           4            0.6472               0.3528    ✅ Consistent   \n",
       "4           5            0.0624               0.9376  ❌ Hallucinated   \n",
       "5           6            0.0099               0.9901  ❌ Hallucinated   \n",
       "6           7            0.0044               0.9956  ❌ Hallucinated   \n",
       "7           8            0.0052               0.9948  ❌ Hallucinated   \n",
       "8           9            0.2404               0.7596  ❌ Hallucinated   \n",
       "9          10            0.8093               0.1907    ✅ Consistent   \n",
       "10         11            0.0139               0.9861  ❌ Hallucinated   \n",
       "11         12            0.0779               0.9221  ❌ Hallucinated   \n",
       "12         13            0.0641               0.9359  ❌ Hallucinated   \n",
       "13         14            0.0070               0.9930  ❌ Hallucinated   \n",
       "14         15            0.1086               0.8914  ❌ Hallucinated   \n",
       "\n",
       "                                     Generated Answer  \n",
       "0   The idea behind multi-headed attention is simp...  \n",
       "1   In our implementation, we first split the orig...  \n",
       "2   We do this by applying linear transformations ...  \n",
       "3   The multi-headed attention mechanism helps us ...  \n",
       "4   This can be especially useful when dealing wit...  \n",
       "5   To illustrate this idea, imagine you have an i...  \n",
       "6   If you were trying to identify what's happenin...  \n",
       "7   Each head has its own set of learnable paramet...  \n",
       "8   Multi-Head Attention is a key component of Tra...  \n",
       "9   This enables the model to capture various aspe...  \n",
       "10  In traditional self-attention mechanisms, the ...  \n",
       "11  However, with multi-head attention, we introdu...  \n",
       "12  Each head learns to weigh these features diffe...  \n",
       "13  By dividing the input sequence into several sm...  \n",
       "14  These shared parameters allow the model to lea...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_final2 = pd.DataFrame(final_results2)\n",
    "display(df_final2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be869167-d92e-4805-b790-c69a70c78fde",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# General Observation\n",
    "Out of 15 generated answers, only 2 were labeled as consistent (4, 10), while the remaining 13 were flagged as hallucinated.<br>\n",
    "Some answers had very high hallucination scores (e.g., >0.99), indicating that the model was quite certain about their inconsistency.<br>\n",
    "A few answers had moderate consistency scores , which might suggest borderline cases depending on the threshold.<br><br>\n",
    "\n",
    "\n",
    "# Interpretation\n",
    "Vectara’s classifier is very strict. It likely flags any additional information, analogies, or simplified examples (even if factually correct) as hallucinated, since these elements aren’t directly verifiable from the original reference text.<br>\n",
    "Answer 4 and 10, marked as consistent, seem to stay closer to the core content and likely rephrase or slightly expand without introducing external analogies or examples.<br>\n",
    "Answers like 1, 2, and 5 might only be expanding the original idea (e.g., mentioning linear transformations), but Vectara considers this expansion potentially risky if not grounded explicitly in the reference text.\n",
    "\n",
    "# Conclusion\n",
    "Vectara might be useful in cases where strict classification is appropriate and needed. Otherwise it might confuse human agents with its classification, marking factually correct answers and sentences as wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2fe6b8-96b9-43d4-87ac-a5fd00903f26",
   "metadata": {},
   "source": [
    "<hr style=\"height:10px;border-width:0;color:#CCD7E9;background-color:#CCD7E9\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
