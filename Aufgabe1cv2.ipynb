{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWwh4wTjWhDS",
        "outputId": "02e372f0-ab30-430b-9f14-bbf72aa4259e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Text with Controlled Creativity (Predictable) ===\n",
            "\n",
            "Tell me something about Multi head attention and make it as simple as possible.  I'm trying to understand how the brain works in this situation\n",
            "You're looking for a multi-modal analysis, which is why you need to have multiple input sources (eyes). You can't ask us what we would want from a task without knowing that your brain can do it.\n",
            "Well my question was directed at multi modal analysis so I didn't mean any disrespect towards anyone's answers just curious if there were any methods of doing it with only eyes or whatnot.. Thanks though!\n",
            "\n",
            "=== Text with High Creativity (More Random) ===\n",
            "\n",
            "Tell me something about Multi head attention and make it as simple as possible, with only one channel visible!\n",
            "\n",
            "Head of Multi, Attention & Multitasking (HeadAM), as you refer to this part of TensorFlow Multi-Head Attention module helps you easily manage the heads. There are only two (2) types: the ‘Pipe Head‘ module and the ‘InputHead‘ module (but each one is totally customizable). What do you mean by HeadAM? HeadAM is a framework that manages and controls head of multiple models of the same model. I will show you how. Here is a flow that illustrates a head of different types.\n",
            "\n",
            "Each channel of HeadAM is usually only exposed once\n",
            "\n",
            "=== Analysis of Generated Text ===\n",
            "\n",
            "Original Text:\n",
            "Tell me something about Multi head attention and make it as simple as possible\n",
            "\n",
            "Generated Text 1:\n",
            "Tell me something about Multi head attention and make it as simple as possible.  I'm trying to understand how the brain works in this situation\n",
            "You're looking for a multi-modal analysis, which is why you need to have multiple input sources (eyes). You can't ask us what we would want from a task without knowing that your brain can do it.\n",
            "Well my question was directed at multi modal analysis so I didn't mean any disrespect towards anyone's answers just curious if there were any methods of doing it with only eyes or whatnot.. Thanks though!\n",
            "\n",
            "Generated Text 2:\n",
            "Tell me something about Multi head attention and make it as simple as possible, with only one channel visible!\n",
            "\n",
            "Head of Multi, Attention & Multitasking (HeadAM), as you refer to this part of TensorFlow Multi-Head Attention module helps you easily manage the heads. There are only two (2) types: the ‘Pipe Head‘ module and the ‘InputHead‘ module (but each one is totally customizable). What do you mean by HeadAM? HeadAM is a framework that manages and controls head of multiple models of the same model. I will show you how. Here is a flow that illustrates a head of different types.\n",
            "\n",
            "Each channel of HeadAM is usually only exposed once\n",
            "\n",
            "Discussion:\n",
            "- How does the generated text compare to the original research content?\n",
            "- Is the AI-generated text coherent and informative?\n",
            "- How do different parameters influence fluency and accuracy?\n",
            "- Does the model introduce errors or go off-topic?\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load a pre-trained text generation model from Hugging Face\n",
        "model_name = \"facebook/opt-1.3b\"  # You can choose other models like \"EleutherAI/gpt-neo-1.3B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(prompt, max_length=100, temperature=1.0, top_k=50, top_p=0.95, repetition_penalty=1.0):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        do_sample=True\n",
        "    )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Example prompt from research context\n",
        "research_prompt =  \"\"\"Tell me something about Multi head attention and make it as simple as possible\"\"\"\n",
        "\n",
        "# Generate text with different parameters\n",
        "output_1 = generate_text(research_prompt, max_length=150, temperature=0.7, top_k=40, top_p=0.9, repetition_penalty=1.2)\n",
        "output_2 = generate_text(research_prompt, max_length=150, temperature=1.2, top_k=100, top_p=0.98, repetition_penalty=1.0)\n",
        "\n",
        "# Print results\n",
        "print(\"\\n=== Text with Controlled Creativity (Predictable) ===\\n\")\n",
        "print(output_1)\n",
        "print(\"\\n=== Text with High Creativity (More Random) ===\\n\")\n",
        "print(output_2)\n",
        "\n",
        "# Analysis Section\n",
        "def analyze_results(original_text, generated_texts):\n",
        "    print(\"\\n=== Analysis of Generated Text ===\\n\")\n",
        "    print(\"Original Text:\")\n",
        "    print(original_text)\n",
        "\n",
        "\n",
        "# Run analysis\n",
        "analyze_results(research_prompt, [output_1, output_2])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load a pre-trained text generation model from Hugging Face\n",
        "model_name = \"facebook/opt-1.3b\"  # You can choose other models like \"EleutherAI/gpt-neo-1.3B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(prompt, max_length=100, temperature=1.0, top_k=50, top_p=0.95, repetition_penalty=1.0):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        do_sample=True\n",
        "    )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Example prompt from research context\n",
        "research_prompt =  \"\"\"Please simplify the following excerpt of a paper for me and tell me more about the topic:\n",
        " Multi-head attention allows the model to jointly attend to information from different representation\n",
        "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
        "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
        "dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
        "is similar to that of single-head attention with full dimensionality.\"\"\"\n",
        "\n",
        "\n",
        "# Generate text with different parameters\n",
        "output_1 = generate_text(research_prompt, max_length=450, temperature=0.7, top_k=40, top_p=0.9, repetition_penalty=1.2)\n",
        "output_2 = generate_text(research_prompt, max_length=450, temperature=1.2, top_k=100, top_p=0.98, repetition_penalty=1.0)\n",
        "\n",
        "# Print results\n",
        "print(\"\\n=== Text with Controlled Creativity (Predictable) ===\\n\")\n",
        "print(output_1)\n",
        "print(\"\\n=== Text with High Creativity (More Random) ===\\n\")\n",
        "print(output_2)\n",
        "\n",
        "# Analysis Section\n",
        "def analyze_results(original_text, generated_texts):\n",
        "    print(\"\\n=== Analysis of Generated Text ===\\n\")\n",
        "    print(\"Original Text:\")\n",
        "    print(original_text)\n",
        "\n",
        "\n",
        "# Run analysis\n",
        "analyze_results(research_prompt, [output_1, output_2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uieuv368bI7_",
        "outputId": "8c8d731f-c5b0-46f0-fa96-a7260a001385"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Text with Controlled Creativity (Predictable) ===\n",
            "\n",
            "Please simplify the following excerpt of a paper for me and tell me more about the topic:\n",
            " Multi-head attention allows the model to jointly attend to information from different representation\n",
            "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
            "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
            "dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
            "is similar to that of single-head attention with full dimensionality. This is achieved by\n",
            "allocating the attention resources among all of the heads equally, rather than allowing them to\n",
            "be allocated according to the number of edges in each subspace. As such, the overall computational\n",
            "cost remains relatively constant while requiring less memory than one would expect if only\n",
            "single-headed attention were used. We show that multi-head attention can be applied to\n",
            "general multi-view tasks as well as simple task models where the same object is presented\n",
            "in two views simultaneously (e.g., 2D color or texture mapping).\n",
            "\n",
            "I want to know what you mean by \"multi-head\" attention, i.e., how many simultaneous\n",
            "attention processes are needed? I've never heard anyone talk about multihued attention being useful\n",
            "for anything other than stereo vision (and even then it's not really an example of multitasking) but\n",
            "maybe someone knows something I don't.\n",
            "\n",
            "If you have read the paper and still don't understand, I recommend watching the video linked below. It might help to explain things better.\n",
            "http://www.youtube.com/watch?v=8m6N1qM0Qrk&feature=player_embedded\n",
            "\n",
            "The concept here seems like a lot of work for just a couple of neurons. Maybe there is some benefit to using multiple processors but so far, I haven't seen any compelling evidence for it.\n",
            "\n",
            "Multi-Head Attention is the ability to process multiple incoming stimuli simultaneously. The idea behind multi-head attention was developed by David Pritchard and David Susskind in their book \"Multitasking\". If you take\n",
            "\n",
            "=== Text with High Creativity (More Random) ===\n",
            "\n",
            "Please simplify the following excerpt of a paper for me and tell me more about the topic:\n",
            " Multi-head attention allows the model to jointly attend to information from different representation\n",
            "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
            "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
            "dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
            "is similar to that of single-head attention with full dimensionality. Head activation\n",
            "and depth must be taken by weight. We propose a new approach,\n",
            "H:LH:R(l) = L:HR(t), G H B O V, which enables a top-based support, or H:LH:R, which makes\n",
            "it unnecessary to know which head the model has been currently assigned. Because heads are\n",
            "polarised from each other to both face orientation and location direction information, the problem\n",
            "can be done efficiently\n",
            "As an extended proof-of-concept, we compute the\n",
            "average head rate by assigning h to l from the model’s state instead of its weight, the\n",
            "formal estimate at h= 0. This allows a smooth scaling effect due to the fact that l= 0\n",
            "(i.e., h=0). Otherwise we suffer from the tradeoff problem under\n",
            "L (h, l). We describe a cost-efficient implementation combining multi-head attention and\n",
            "high-dimensional reinforcement.\n",
            "\n",
            "Please simplify the following excerpt of a paper for me and tell me more about the topic:\n",
            " Multi-head attention allows the model to jointly attend to information from different representation\n",
            "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
            "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
            "dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
            "is similar to that of single-head attention with full dimensionality. Head activation\n",
            "and depth must be taken by weight. We propose a new approach\n",
            "\n",
            "=== Analysis of Generated Text ===\n",
            "\n",
            "Original Text:\n",
            "Please simplify the following excerpt of a paper for me and tell me more about the topic:\n",
            " Multi-head attention allows the model to jointly attend to information from different representation\n",
            "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
            "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
            "dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
            "is similar to that of single-head attention with full dimensionality.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load a pre-trained text generation model from Hugging Face\n",
        "model_name = \"facebook/opt-1.3b\"  # You can choose other models like \"EleutherAI/gpt-neo-1.3B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Function to generate text\n",
        "def generate_text(prompt, max_length=100, temperature=1.0, top_k=50, top_p=0.95, repetition_penalty=1.0):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        do_sample=True\n",
        "    )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Example prompt from research context\n",
        "research_prompt =  \"\"\"Please simplify the following excerpt of a paper for me and tell me more about the topic and connections with bananas:\n",
        " Multi-head attention allows the model to jointly attend to information from different representation\n",
        "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
        "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
        "dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
        "is similar to that of single-head attention with full dimensionality.\"\"\"\n",
        "\n",
        "\n",
        "# Generate text with different parameters\n",
        "output_1 = generate_text(research_prompt, max_length=650, temperature=0.7, top_k=40, top_p=0.9, repetition_penalty=1.2)\n",
        "output_2 = generate_text(research_prompt, max_length=650, temperature=1.2, top_k=100, top_p=0.98, repetition_penalty=1.0)\n",
        "\n",
        "# Print results\n",
        "print(\"\\n=== Text with Controlled Creativity (Predictable) ===\\n\")\n",
        "print(output_1)\n",
        "print(\"\\n=== Text with High Creativity (More Random) ===\\n\")\n",
        "print(output_2)\n",
        "\n",
        "# Analysis Section\n",
        "def analyze_results(original_text, generated_texts):\n",
        "    print(\"\\n=== Analysis of Generated Text ===\\n\")\n",
        "    print(\"Original Text:\")\n",
        "    print(original_text)\n",
        "\n",
        "\n",
        "# Run analysis\n",
        "analyze_results(research_prompt, [output_1, output_2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UoKP7BBbgsou",
        "outputId": "b5bcd31c-1cfc-4f58-d536-bae9eb26ac2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Text with Controlled Creativity (Predictable) ===\n",
            "\n",
            "Please simplify the following excerpt of a paper for me and tell me more about the topic and connections with bananas:\n",
            " Multi-head attention allows the model to jointly attend to information from different representation\n",
            "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
            "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
            "dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
            "is similar to that of single-head attention with full dimensionality. This is due to the fact that\n",
            "we only need one attention channel per data point, instead of two. As expected, it takes less time\n",
            "to process multi-head data than it does to process single-head data, but the efficiency gains are small.\n",
            "\n",
            "What are your thoughts on this? Do you think there is any value in using multiple headings (in addition to h) when processing data? Any other suggestions would be appreciated!\n",
            "\n",
            "=== Text with High Creativity (More Random) ===\n",
            "\n",
            "Please simplify the following excerpt of a paper for me and tell me more about the topic and connections with bananas:\n",
            " Multi-head attention allows the model to jointly attend to information from different representation\n",
            "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
            "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
            "dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
            "is similar to that of single-head attention with full dimensionality.\n",
            "\n",
            "Re: Basic requirements for a research question\n",
            "\n",
            "When it comes to abstract thinking questions, it has a different name than that of, say, \"how should we deal with people\n",
            "who don’t like science?\". Its name is more along the lines of \"How can we change the way\n",
            "individual’s think about science, given so many scientific\n",
            "informations to digest?\".\n",
            "\n",
            "Now, as I am not the greatest speaker of English, I do have some difficulty\n",
            "wording what needs to be changed. I hope this is not all too complicated\n",
            "for to your eye. Here we should start from a fairly easy and obvious truth: people do not think about ideas\n",
            "or objects the way that we think about science. We think about ideas or objects thinking way before\n",
            "“I” think about them. We can make different scientific observations of the objects and look back at “I”\n",
            "for how they got “there”. However, nobody thinks about abstract ideas like the way that we think of “I”\n",
            "thinking. It would take quite a big brain to contemplate them as abstract ideas.\n",
            "\n",
            "What we want to do to turn this situation is to change that state by introducing abstract thinking\n",
            "elements into those processes and processes. So to do this, we get h = 8 layers from which one takes\n",
            "h=d = dmodel h=64. But that sounds a little mathematical. If we want to actually change anything about that\n",
            "state, why not take to account dk = m = c model for the data stream between h(i,j). And therefore\n",
            "use d*= m*=c (dk=M)\n",
            "(c=C)\n",
            "for the streams of h=12, h(t)=t/12, and those of h(t=7), h(m)+f, and the data between the m (with C) and\n",
            "C models.\n",
            "\n",
            "There are some further questions that are kind of hard to figure out for people outside\n",
            "the field, one of which is basically do any of the heads come from one process, say\n",
            "h=12, or can they be of various types, is that also mathematical, all of those types\n",
            "concerning inputs, output, thinking about other process such as “I” think:\n",
            "is that also a way that we think:\n",
            "it takes a very long brain to think abstractly, even harder to think abstractly when that starts thinking how we think\n",
            "about how I think that.\n",
            "\n",
            "In my\n",
            "\n",
            "=== Analysis of Generated Text ===\n",
            "\n",
            "Original Text:\n",
            "Please simplify the following excerpt of a paper for me and tell me more about the topic and connections with bananas:\n",
            " Multi-head attention allows the model to jointly attend to information from different representation\n",
            "subspaces at different positions. With a single attention head, averaging inhibits this.\n",
            "In this work we employ h = 8 parallel attention layers, or heads. For each of these we use\n",
            "dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\n",
            "is similar to that of single-head attention with full dimensionality.\n"
          ]
        }
      ]
    }
  ]
}